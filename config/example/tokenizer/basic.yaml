#load_path: "./experiments/test/ha/model"

tokenizer_config:
  model:
    type: "unigram"
  vocab_size: "auto"
  use_sp_backend: true
  sp_kwargs:
    byte_fallback: false
  special_tokens:
    pad_token: "[PAD]"
    unk_token: "[UNK]"
    bos_token: "[BOS]"
    eos_token: "[EOS]"
    cls_token: "[CLS]"
    sep_token: "[SEP]"
    mask_token: "[MASK]"

export: true
push_to_hub: true