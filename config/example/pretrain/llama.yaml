load_path: "meta-llama/Llama-2-7b-hf"

training_parameters:
  model_type: "llama"
  task: "clm"
  max_length: 512
  num_train_epochs: 3
  batch_size: 8
  lr: 3e-4
  padding_strategy: "longest"
  grad_accumulation_steps: 1
  mixed_precision: "bf16"
  quantize_4bit: true
  peft_config:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules:
      - "q_proj"
      - "v_proj"
    bias: "none"

test_path: "WikiQuality/raw_wiki"
export: true
push_to_hub: true
checkpoint: true