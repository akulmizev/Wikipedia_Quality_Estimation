#load_path: "./experiments/test/ha/model"

tokenizer_config:
  model:
    type: "unigram"
  trainer:
    type: "unigram"
  normalizer:
  - type: "nfkc"
  pre_tokenizer:
  - type: "digits"
    individual_digits: false
  - type: "metaspace"
  decoder:
    type: "metaspace"
  vocab_size: "auto"
  special_tokens:
    pad_token: "[PAD]"
    unk_token: "[UNK]"
    bos_token: "[BOS]"
    eos_token: "[EOS]"
    cls_token: "[CLS]"
    sep_token: "[SEP]"
    mask_token: "[MASK]"

export: true
push_to_hub: true