#load_path: "./experiments/test/ha/model"

tokenizer_config:
  model:
    type: "unigram"
  trainer:
    type: "unigram"
  normalizer:
  - type: "nfkc"
  - type: "replace"
    pattern: " "
    content: "‚ñÅ"
  pre_tokenizer:
  - type: "digits"
    individual_digits: false
  - type: "metaspace"
  decoder:
    type: "metaspace"
  vocab_size: "auto"

export: true
push_to_hub: false