#load_path: "WikiQuality/test"

parameters:
  vocab_size: "auto"
  model: "unigram"
  normalizer: "nfkc"
  pre_tokenizer: [
    "metaspace",
    "digits",
    "bert"
  ]
  decoder: "metaspace"
  trainer: "unigram"
  post_processor: true
  special_tokens: {
    mask_token: "[MASK]",
    bos_token: "[BOS]",
    eos_token: "[EOS]",
    sep_token: "[SEP]",
    pad_token: "[PAD]",
    unk_token: "[UNK]",
    cls_token: "[CLS]"
  }

export: true
push_to_hub: true