tokenizer:
  do_tokenize: true
  model: "unigram"
  normalizer: "nfkc"
  pre_tokenizer: [
    "unicode_scripts",
    "digits",
    "metaspace"
  ]
  decoder: "metaspace"
  trainer: "unigram"
  post_processor: true
  special_tokens: [
    "[PAD]",
    "[BOS]",
    "[EOS]",
    "[MASK]",
    "[CLS]",
    "[SEP]",
    "[UNK]"
  ]
  unk_token: "[UNK]"
  vocab_size: "auto"
  output_dir: "../tokenizers/"

data:
  import:
    do_import: false
    path: "WikiQuality/as.filtered"
  pre_filter:
    do_pre_filter: false
    script_regex: true
    lang_id: true
    char_cutoff: 15
  partition:
    do_partition: true
    partition_type: "mean_cutoff"
    partition_metric: "length"
    higher_is_better: true
    quality: true
  export:
    do_export: true
    export_type: "hub"
    path: "WikiQuality"

model:
  pretrain:
    do_pretrain: true
    from_config: true
    model_type: "deberta"
    config_path: "../config/model/deberta/tiny.json"
    max_seq_length: 512
    mask_prob: 0.4
    max_predictions_per_seq: 20
    do_whole_word_mask: true
    do_lower_case: false
    output_dir: "../models/"