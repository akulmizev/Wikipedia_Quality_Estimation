experiment:
  experiment_id: "new_config_debugger"
  wandb_entity: "wikiquality"
  local_path: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/WikiQuality/experiments"
  hub_path: "WikiQuality"

data:
  load:
#    method: "raw"
    method: "hub"
    path: "WikiQuality/pre_filtered"
#  pre_filter:
#    script_regex: true
#    lang_id: false
#    char_cutoff: 15
#  split:
#    train: 0.95
#    test: 0.05
#    seed: 42
#    shuffle: true
  export: false
#  push_to_hub: true

tokenizer:
  load:
    method: "hub"
    path: "WikiQuality/pre_filtered"
#  parameters:
#    model: "unigram"
#    normalizer: "nfkc"
#    pre_tokenizer: [
#      "metaspace",
#      "bert"
#    ]
#    decoder: "metaspace"
#    trainer: "unigram"
#    post_processor: true
#    special_tokens: {
#      mask_token: "[MASK]",
#      bos_token: "[BOS]",
#      eos_token: "[EOS]",
#      sep_token: "[SEP]",
#      pad_token: "[PAD]",
#      unk_token: "[UNK]",
#      cls_token: "[CLS]"
#    }
#    unk_token: "[UNK]"
#    vocab_size: "auto"
#    min_frequency: 2
  export: false
  push_to_hub: false

pretrain:
  load:
    method: "config"
    path: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/config/model/tiny_deberta/config.  json"
  do_train: true
  task: "mlm"
  training_parameters:
    model_type: "deberta"
    max_length: 512
    mask_prob: 0.4
    num_train_epochs: 25
    batch_size: 8
    lr: 1e-03 #DO NOT CHANGE THIS
    eval_steps: 250
  test_data:
    method: "hub"
    path: "WikiQuality/raw_wiki"
  export: true
  push_to_hub: true

#finetune:
#  model_type: "deberta"
#  from_pretrained: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/models/deberta_tiny_mlm"
#  num_train_epochs: 1
#  batch_size: 8
#  lr: 5e-5
#  task: "ner"
#  dataset: "ai4bharat/naamapadam"
