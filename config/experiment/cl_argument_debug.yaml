experiment:
  experiment_id: "raw_wiki_lr"
  wandb_entity: "wikiquality"
  local_path: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/experiments"
  hub_path: "WikiQuality"

#data:
#  load:
#    method: "hub" #or pretrained
#    path: "WikiQuality/pre_filtered"
#  export: false #this needs to be in the config, even when loading from hub and not doing anything else
#  push_to_hub: false

#tokenizer:
#  load:
#    method: "config"
##    path: "WikiQuality/pre_filtered"
#  parameters:
#    model: "unigram"
#    normalizer: "nfkc"
#    pre_tokenizer: [
#      "metaspace",
#      "unicode_scripts",
#      "digits"
#    ]
#    decoder: "metaspace"
#    trainer: "unigram"
#    post_processor: true
#    special_tokens: {
#      mask_token: "[MASK]",
#      bos_token: "[BOS]",
#      eos_token: "[EOS]",
#      sep_token: "[SEP]",
#      pad_token: "[PAD]",
#      unk_token: "[UNK]",
#      cls_token: "[CLS]"
#    }
#    unk_token: "[UNK]"
#    vocab_size: "auto"
#    min_frequency: 2
#  export: false

#pretrain:
#  load:
#    method: "config"
##    path: "WikiQuality/ak_testing_2"
#    path: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/config/model/deberta/alt.json"
#  do_train: true
#  task: "mlm"
#  training_parameters:
#    model_type: "deberta"
#    max_length: 512
#    mask_prob: 0.4
#    num_train_epochs: 3
#    batch_size: 8
#    lr: 1e-03
#    eval_steps: 1000
#  test_data:
#    method: "hub"
#    path: "WikiQuality/raw_wiki"
#  export: true
#  push_to_hub: true

#finetune:
#  load:
#    method: "pretrained"
#    path: "WikiQuality/config_testing"
#  do_train: true #this gives an error
#  task: "ner"
#  dataset_path: "WikiQuality/ner"
#  training_parameters:
#    model_type: "deberta"
#    max_length: 512
#    num_train_epochs: 10
#    batch_size: 32
#    lr: 3e-03
#  export: false
#  push_to_hub: false