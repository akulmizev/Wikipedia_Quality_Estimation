experiment:
  experiment_id: "length.high_quality.pos"
  wandb_entity: "wikiquality"
  local_path: "/home/akulmizev/Repos/Wikipedia_Quality_Estimation/experiments"
  hub_path: "WikiQuality"

data:
  load:
    method: "hub"
    path: "WikiQuality/raw_wiki"
  pre_filter:
    script_regex: true
    lang_id: true
    char_cutoff: 15
  partition:
    method: "balanced_chars"
    metric_type: "length"
    quality: true
    all_partitions_join_method: "intersection" # intersection or union
  split:
    train: 0.9
    test : 0.1
    seed: 42
    shuffle: true
  export: true
  push_to_hub: true
  
tokenizer:
  load:
    method: "config"
  parameters:
    model: "unigram"
    normalizer: "nfkc"
    pre_tokenizer: [
      "metaspace",
      "bert"
    ]
    decoder: "metaspace"
    trainer: "unigram"
    post_processor: true
    special_tokens: {
      mask_token: "[MASK]",
      bos_token: "[BOS]",
      eos_token: "[EOS]",
      sep_token: "[SEP]",
      pad_token: "[PAD]",
      unk_token: "[UNK]",
      cls_token: "[CLS]"
    }
    unk_token: "[UNK]"
    vocab_size: "auto"
    min_frequency: 2
  export: true
  push_to_hub: true

pretrain:
  load:
    method: "config"
    path: "/lustre1/project/stg_00120/kushal/Wikipedia/Wikipedia_Quality_Estimation/config/model/tiny_deberta/config.json"
  do_train: true
  task: "mlm"
  training_parameters:
    model_type: "deberta"
    max_length: 512
    mask_prob: 0.4
    num_train_epochs: 150
    batch_size: 8
    lr: 1e-03 #DO NOT CHANGE THIS
    eval_steps: 100
  test_data:
    method: "hub"
    path: "WikiQuality/raw_wiki"
  export: true
  push_to_hub: true

finetune:
  load:
    method: "pretrained"
    path: "WikiQuality/length.high_quality"
  do_train: true 
  task: "pos"
  dataset_path: "WikiQuality/pos"
  training_parameters:
    model_type: "deberta"
    max_length: 512
    num_train_epochs: 10
    batch_size: 32
    lr: 3e-03
  export: true
  push_to_hub: true
